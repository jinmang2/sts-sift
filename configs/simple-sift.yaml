ModelTrainingArguments:
    # projects
    model_path: null
    wandb_project: Scatterlab-STS
    apply_sift: True
    training_mode: sts
    k_fold: 5

    # ðŸ¤— TrainingArguments
    output_dir: outputs
    seed: 42
    save_total_limit: 3
    num_train_epochs: 5
    learning_rate: 5e-5
    per_device_train_batch_size: 64
    per_device_eval_batch_size: 64
    gradient_accumulation_steps: 1
    lr_scheduler_type: cosine
    weight_decay: 0.01
    logging_dir: ./logs
    evaluation_strategy: epoch
    save_strategy: epoch
    metric_for_best_model: accuracy
    fp16: True
    fp16_opt_level: O1
    load_best_model_at_end: True
    report_to: wandb
    run_name: scatter-lab-sts-sift

    # models (roberta)
    vocab_size: 30006
    hidden_size: 512
    num_hidden_layers: 6
    num_attention_heads: 8
    intermediate_size: 2048
